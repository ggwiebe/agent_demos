{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330338f9-d5dc-493c-ab15-74ca7ab8f4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# README\n",
    "\n",
    "**IMPORTANT: This notebook has cached outputs so you can demo WITHOUT needing to re-run.  See [xx]() for a clone of this notebook w/out the demo script, but with the outputs for demo'ing.**\n",
    "\n",
    "How to give this demo:\n",
    "1. **Tee up the problem:** Developers struggle to get their SMEs to create evaluation sets - it takes weeks to months, and the resulting data is often poor quality because SMEs struggle to create diverse questions from scratch.\n",
    "2. **Tee up the solution:** In partnership with Mosaic AI Research, we created the Agent Evaluation Synthetic API to address this challenge - it allows you to create a diverse, representative evaluation set based on your documents. With this data, you can immediately start iterating on quality!\n",
    "3. **Tee up the demo:** Today, I'll walk you through the process of generating synthetic data, comparing the quality/cost/latency of a function-calling Agent with a vector search retrieval tool between several LLMs, and then deploying the best Agent to either production or to a web-based chat app so your SMEs can provide feedback on its quality.  We'll do this with the Databricks documentation.\n",
    "4. **Give the demo:** (follow talk track inline)\n",
    "\n",
    "Quick links to get UIs w/ visuals:\n",
    "* [**MLflow Evaluation UI:** see the generated questions & LLM judge quality evaluation]()\n",
    "* [**MLflow Runs UI:** see metrics comparing Agent versions on quality/cost/latency]()\n",
    "* [**Review App:** web-based chat app for SMEs]()\n",
    "\n",
    "*IMPORTANT: The Review App is scaled to zero.  Go ask a question ~15 mins before your demo to let it warm  up!*\n",
    "\n",
    "FAQ:\n",
    "- Q: How do I get my SMEs to review the synthetic data to ensure it's accurate?\n",
    "  - A: We have the evaluation set review UI, which lets SMEs review and edit the evaluation set. If you are interested, we can enroll you in the Private Preview.\n",
    "- Q: How do I tune the generated questions for my use case?\n",
    "  - A: You can use the `guidelines` parameter which allows you to provide English instructions about the style of question you want, the target user persona, and more. The API will take these instructions into account when generating data.\n",
    "- Q: How many questions should I generate?\n",
    "  - A: We suggest at least 2 - 3 questions per document.\n",
    "\n",
    "\n",
    "**If you want to modify this notebook, please clone [this copy]() that has user_name paramet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266d5662-d9ba-4635-a862-4041c746a0db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install packages"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq \"git+https://github.com/mlflow/mlflow.git@master\" \"https://ml-team-public-read.s3.us-west-2.amazonaws.com/wheels/rag-studio/staging/databricks_agents-0.8.1.dev0-py3-none-any.whl\" \"https://ml-team-public-read.s3.us-west-2.amazonaws.com/wheels/managed-evals/staging/databricks_managed_evals-latest-py3-none-any.whl\" databricks-vectorsearch databricks-sdk[openai] \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f106f52-95af-4ef8-af7c-03de70263f7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Settings for the demo"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Get current user's name & email to ensure each user doesn't over-write other user's outputs\n",
    "w = WorkspaceClient()\n",
    "user_email = w.current_user.me().user_name\n",
    "user_name = user_email.split(\"@\")[0].replace(\".\", \"_\")\n",
    "\n",
    "experiment = mlflow.set_experiment(f\"/Users/{user_email}/agents-demo-experiment\")\n",
    "\n",
    "synthetic_evaluation_set_delta_table = (\n",
    "    f\"agents_demo.synthetic_data.db_docs_synthetic_eval_set__{user_name}\"\n",
    ")\n",
    "\n",
    "managed_eval_delta_table = (\n",
    "    f\"agents_demo.synthetic_data.db_docs_managed_eval_set__{user_name}\"\n",
    ")\n",
    "\n",
    "uc_model_name = f\"agents_demo.synthetic_data.db_docs__{user_name}\"\n",
    "\n",
    "print(f\"User: {user_name}\")\n",
    "print()\n",
    "print(f\"MLflow Experiment: {experiment.name}\")\n",
    "print()\n",
    "print(f\"Synthetic Data output Delta Table: {synthetic_evaluation_set_delta_table}\")\n",
    "print(f\"Managed Evaluation Set Delta Table: {managed_eval_delta_table}\")\n",
    "print(f\"Unity Catalog Model: {uc_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe40985-a392-49ed-a170-004814fd17ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate synthetic evaluation set\n",
    "\n",
    "DEMO SCRIPT: Here, I'll pass my documents (Databricks docs in this demo) to the Synthetic API along with some guidance to tune the generated questions for my use case.  The API will generate synthetic questions & ground truth responses using our propietary synthetic data pipeline that we built in partnership with Mosaic AI Research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35f8c0a-a3ad-4236-b2a1-27d8464b7c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "def generate_evals_df(\n",
    "    docs: Union[pd.DataFrame, \"pyspark.sql.DataFrame\"], *,\n",
    "    num_questions_per_doc: int = 3,\n",
    "    guidelines: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run the synthetic generation pipeline to generate evaluations for a given set of documents.\n",
    "    Generated evaluation set can be used with Databricks Agent Evaluation (https://docs.databricks.com/en/generative-ai/agent-evaluation/evaluate-agent.html).\n",
    "\n",
    "    :param docs: A pandas/Spark DataFrame with a string column `content` and a string `doc_uri` column.\n",
    "    :param num_questions_per_doc: The number of questions to generate for each document. Default is 3.\n",
    "    :param guidelines: Optional guidelines to guide the question generation.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a10a11-5d26-4222-8247-13ea1d2df3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents.eval import generate_evals_df\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Get parsed documents\n",
    "df = (\n",
    "    spark.table(\"agents_demo.agents.db_docs_docs__initial\")\n",
    "    .orderBy(rand())\n",
    "    .withColumnRenamed(\"doc_content\", \"content\")\n",
    "    .limit(10) # Only do 10 questions for the demo\n",
    ")\n",
    "\n",
    "# Optional guideline\n",
    "\n",
    "# NOTE: The guidelines you provide are a free-form string. The markdown string below is the suggested formatting for the set of guidelines, however you are free to add your sections here. Note that this will be prompt-engineering an LLM that generates the synthetic data, so you may have to iterate on these guidelines before you get the results you desire.\n",
    "guidelines = \"\"\"\n",
    "# Task Description\n",
    "You are generating an evaluation dataset which will be used to test a RAG chatbot on its ability to answer questions about Databricks documentation, providing support for Databricks APIs and its UI.\n",
    "\n",
    "# Content Guidelines\n",
    "- Address scenarios where data engineers are trying to understand the product capabilities\n",
    "- Simulate real-world scenarios a data engineer may occur when writing data pipelines\n",
    "\n",
    "# Example questions\n",
    "- what is in a good eval set\n",
    "- saving files in uc volume\n",
    "- spark sql join\n",
    "- How do I add a secret?\n",
    "\n",
    "# Style Guidelines\n",
    "- Questions should be succinct, and human-like.\n",
    "\n",
    "# Personas\n",
    "- A Data Scientist using Databricks.\n",
    "\"\"\"\n",
    "\n",
    "# Generate 1 question for each document\n",
    "synthetic_data = generate_evals_df(\n",
    "    docs=df, guidelines=guidelines, num_questions_per_doc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b299e82e-39b4-4cb5-b62e-0184241c6407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DEMO SCRIPT: Now, let's look at a few of the questions.  You can see that for each document, we generated a synthetic question and the expected facts (ground truth) that the Agent must generate to get the questions correct.  We generate JUST the facts, rather than a fully written answer, since this helps the accuracy of the propietary LLM judges we will see later.  If your SMEs will review these questions, having just the facts, versus a generated response, helps makes them more efficient in their review.\n",
    "\n",
    "*Note: Click to the visualization tab to see a pretty rendering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0137f7de-d445-44d8-9bfd-463f1d4d2bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "synthetic_data_spark_df = spark.createDataFrame(synthetic_data)\ndisplay(synthetic_data_spark_df)\n\nresulting_delta_table = f\"agents_demo.synthetic_data.db_docs_synthetic_eval_set__{user_name}\"\nsynthetic_data_spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(resulting_delta_table)",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "DETAILS"
         },
         {
          "key": "options",
          "value": {
           "columns": [
            {
             "name": "request_id",
             "title": "request_id",
             "type": "string"
            },
            {
             "name": "request",
             "title": "request",
             "type": "string"
            },
            {
             "name": "expected_retrieved_context",
             "title": "expected_retrieved_context",
             "type": "string"
            },
            {
             "name": "expected_facts",
             "title": "expected_facts",
             "type": "string"
            }
           ],
           "version": 1
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "48bc389c-680e-437a-ac6d-b6061365fc58",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthetic_data_spark_df = spark.createDataFrame(synthetic_data)\n",
    "\n",
    "# Display generated questions/ground truth\n",
    "display(\n",
    "    synthetic_data_spark_df.select(\n",
    "        \"request\", \"expected_facts\", \"expected_retrieved_context.doc_uri\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to Delta Table\n",
    "synthetic_data_spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    synthetic_evaluation_set_delta_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6fcf50e-c693-4cc6-946e-1eef29cc2f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the evaluation set to evaluate a RAG Agent\n",
    "\n",
    "DEMO SCRIPT: Now, let's use this synthetic evaluation set to evaluate the quality of our Agent.  Here, I'll use a function-calling Agent with a vector search Retriever that I grabbed from our [AI Cookbook](https://ai-cookbook.io).  Before this call, I built a vector index of the Databricks docs using Vector Search.  Let's quickly look at the MLflow Trace to see what this Agent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac66dc3f-c9b6-4a6c-baa7-ab45dff02dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./function_calling_agent_openai_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a4cbd0-5dae-4a3b-a797-bb5f194f86ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "OPTIONAL DEMO SCRIPT (for advanced teams only, hide cell for others): Here, we can inspect the Agent's configuration.  To improve the agent's quality, you'll tune these parameters, along with your vector index's data pipeline and the agent's code itself.  Note that these parameters are just a starting point - Agent Framework allows you full control over your Agent's code and config which allows you to achieve production-ready quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b31ce4b-f810-49ee-8ecd-52098a382b01",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optionally, inspect the Agent's configuration"
    }
   },
   "outputs": [],
   "source": [
    "# Pydantic class to make configuration easiser to use.  Developers can use this, Python dictionaries or YAML files for their configuration.\n",
    "from agent_config import (\n",
    "    AgentConfig,\n",
    "    FunctionCallingLLMConfig,\n",
    "    LLMParametersConfig,\n",
    "    RetrieverToolConfig,\n",
    "    RetrieverParametersConfig,\n",
    "    RetrieverSchemaConfig,\n",
    ")\n",
    "import yaml\n",
    "\n",
    "retriever_config = RetrieverToolConfig(\n",
    "    vector_search_index=\"agents_demo.agents.db_docs_docs_chunked_index__initial\",  # UC Vector Search index\n",
    "    vector_search_schema=RetrieverSchemaConfig(\n",
    "        primary_key=\"chunk_id\",\n",
    "        chunk_text=\"content_chunked\",\n",
    "        document_uri=\"doc_uri\",\n",
    "        additional_metadata_columns=[],\n",
    "    ),\n",
    "    vector_search_parameters=RetrieverParametersConfig(\n",
    "        num_results=5,\n",
    "        query_type=\"ann\",  # Type of search: ann or hybrid\n",
    "    ),\n",
    "    vector_search_threshold=0.0,\n",
    "    # Tool prompt templates\n",
    "    chunk_template=\"Passage text: {chunk_text}\\nPassage metadata: {metadata}\\n\\n\",\n",
    "    prompt_template=\"\"\"Use the following pieces of retrieved context to answer the question.\\nOnly use the passages from context that are relevant to the query to answer the question, ignore the irrelevant passages.  When responding, cite your source, referring to the passage by the columns in the passage's metadata.\\n\\nContext: {context}\"\"\",\n",
    "    retriever_query_parameter_prompt=\"query to look up in retriever\",\n",
    "    tool_description_prompt=\"Search for documents that are relevant to a user's query about the Databricks documentation.\",\n",
    "    tool_name=\"retrieve_documents\",\n",
    "    # Retriever internals\n",
    "    tool_class_name=\"VectorSearchRetriever\",\n",
    ")\n",
    "\n",
    "########################\n",
    "#### ✅✏️ LLM configuration\n",
    "########################\n",
    "\n",
    "llm_config = FunctionCallingLLMConfig(\n",
    "    llm_endpoint_name=\"agents-demo-gpt4o\",  # Model serving endpoint\n",
    "    llm_system_prompt_template=(\n",
    "        \"\"\"You are a helpful assistant that answers questions by calling tools.  Provide responses ONLY based on the outputs from tools.  If you do not have a relevant tool for a question, respond with 'Sorry, I'm not trained to answer that question'.\"\"\"\n",
    "    ),  # System prompt template\n",
    "    llm_parameters=LLMParametersConfig(\n",
    "        temperature=0.01, max_tokens=1500\n",
    "    ),  # LLM parameters\n",
    "    tools=[retriever_config],\n",
    ")\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    llm_config=llm_config,\n",
    "    input_example={\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is Agent Evaluation?\",\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "########################\n",
    "##### Dump the configuration to a YAML\n",
    "########################\n",
    "\n",
    "# We dump the dump the Pydantic model to a YAML file because:\n",
    "# 1. MLflow ModelConfig only accepts YAML files or dictionaries\n",
    "# 2. When importing the Agent's code, it needs to read this configuration\n",
    "with open(\"config.yml\", \"w\") as file:\n",
    "    yaml.dump(agent_config.dict(), file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4c9ddb-4e5b-4c23-a033-887b80e53eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vibe_check_query = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": f\"what is agent evaluation?\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Could also be \"databricks-meta-llama-3-1-405b-instruct\" or \"agents-demo-gpt4o\" or \"databricks-meta-llama-3-1-70b-instruct\" or any other Model Serving endpoint\n",
    "agent_config.llm_config.llm_endpoint_name = \"agents-demo-gpt4o-mini\"\n",
    "\n",
    "# Set the retriever tool to use our Vector Search index\n",
    "agent_config.llm_config.tools[\n",
    "    0\n",
    "].vector_search_index = \"agents_demo.agents.db_docs_docs_chunked_index__initial\"\n",
    "\n",
    "# Initialize the agent\n",
    "rag_agent = FunctionCallingAgent(agent_config=agent_config.dict())\n",
    "\n",
    "# Call the agent for the vibe check query\n",
    "output = rag_agent.predict(model_input=vibe_check_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "235125f7-3c2a-44b1-bc9c-c343226f8f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate the Agent's performance on a few LLMs\n",
    "\n",
    "DEMO SCRIPT: Now, let's run Agent Evaluation's propietary LLM judges using the synthetic evaluation set to see the quality/cost/latency of several propietary and open source LLMs.  Our research team has invested signficantly in the quality AND speed of these judges, which we define as how often the judge agrees with humans - these judges outperform competitors such as RAGAS in terms of their quality and speed.  \n",
    "\n",
    "Note that while I am showing the comparison of multiple LLMs, you will use this same approach to compare your experiments with code/config changes to improve quality.  Each iteration is logged to MLflow, so you can quickly come back to the code/config version that worked and deploy it.\n",
    "\n",
    "We can use MLflow Evaluation UI to inspect the individual records & see the judge outputs, including how they identified the root cause of quality issues.  This UI, coupled with the speed of evaluation, help you efficiently test their hypotheses to improve quality, which lets you reach the production quality bar faster. \n",
    "\n",
    "We will use the MLflow Runs UI to compare quality/cost/latency metrics between the LLMs.  This helps you make informed tradeoffs in partnership with your business stakeholders about cost/latency/quality.  Further, you can use this view (or turn it into a Lakeview Dashboard) to provide quantitative updates to your stakeholders so they can follow your progress improving quality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fadc76e4-779a-43b7-847c-4fffc116783a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper function for logging to MLflow"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.resources import (\n",
    "    DatabricksVectorSearchIndex,\n",
    "    DatabricksServingEndpoint,\n",
    ")\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.models.rag_signatures import StringResponse, ChatCompletionRequest, Message\n",
    "import yaml\n",
    "from databricks import agents\n",
    "from databricks import vector_search\n",
    "\n",
    "\n",
    "def log_agent_to_mlflow(agent_config, agent_code_file):\n",
    "    # Add the Databricks resources so that credentials are automatically provisioned by agents.deploy(...)\n",
    "    databricks_resources = [\n",
    "        DatabricksServingEndpoint(\n",
    "            endpoint_name=agent_config.llm_config.llm_endpoint_name\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Add the Databricks resources for the retriever's vector indexes\n",
    "    for tool in agent_config.llm_config.tools:\n",
    "        if type(tool) == RetrieverToolConfig:\n",
    "            databricks_resources.append(\n",
    "                DatabricksVectorSearchIndex(index_name=tool.vector_search_index)\n",
    "            )\n",
    "            index_embedding_model = (\n",
    "                VectorSearchClient(disable_notice=True)\n",
    "                .get_index(index_name=retriever_config.vector_search_index)\n",
    "                .describe()\n",
    "                .get(\"delta_sync_index_spec\")\n",
    "                .get(\"embedding_source_columns\")[0]\n",
    "                .get(\"embedding_model_endpoint_name\")\n",
    "            )\n",
    "            if index_embedding_model is not None:\n",
    "                databricks_resources.append(\n",
    "                    DatabricksServingEndpoint(endpoint_name=index_embedding_model),\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Could not identify the embedding model endpoint resource for {tool.vector_search_index}.  Please manually add the embedding model endpoint to `databricks_resources`.\"\n",
    "                )\n",
    "\n",
    "    # Specify the full path to the Agent notebook\n",
    "    # model_file = \"function_calling_agent_openai_sdk\"\n",
    "    # model_path = os.path.join(os.getcwd(), model_file)\n",
    "\n",
    "    # Log the agent as an MLflow model\n",
    "    return mlflow.pyfunc.log_model(\n",
    "        python_model=agent_code_file,\n",
    "        model_config=agent_config.dict(),\n",
    "        artifact_path=\"agent\",\n",
    "        input_example=agent_config.input_example,\n",
    "        resources=databricks_resources,\n",
    "        signature=ModelSignature(\n",
    "            inputs=ChatCompletionRequest(),\n",
    "            outputs=StringResponse(),\n",
    "        ),\n",
    "        extra_pip_requirements=[\n",
    "            \"databricks-agents\",\n",
    "            \"databricks-vectorsearch\",\n",
    "            \"mlflow\", \n",
    "            \"databricks-sdk[openai]\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f44805c-e0da-4a92-b9d4-9aeb42cb9bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.metrics.genai import make_genai_metric_from_prompt\n",
    "import os\n",
    "\n",
    "# Synthetic data from earlier\n",
    "synthetic_data_df = spark.table(synthetic_evaluation_set_delta_table).toPandas()\n",
    "\n",
    "# Add a custom LLM judge to asses use-case specific requirements\n",
    "no_pii_prompt = \"\"\"\n",
    "Your task is to determine whether the content includes PII information (personally identifiable information).\n",
    "\n",
    "You should output a 5 if there is no PII, a 1 if there is PII. This was the content: '{response}'\"\"\"\n",
    "\n",
    "no_pii = make_genai_metric_from_prompt(\n",
    "    name=\"no_pii\",\n",
    "    judge_prompt=no_pii_prompt,\n",
    "    model=\"endpoints:/agents-demo-gpt4o\",\n",
    "    metric_metadata={\"assessment_type\": \"ANSWER\"},\n",
    ")\n",
    "\n",
    "# Compare several LLMs.  This can be ANY Model Serving endpoint, including an OpenAI External Model.\n",
    "model_endpoints_to_test = [\n",
    "    # \"databricks-meta-llama-3-1-405b-instruct\",\n",
    "    \"agents-demo-gpt4o\",\n",
    "    \"agents-demo-gpt4o-mini\",\n",
    "    # \"databricks-meta-llama-3-1-70b-instruct\",\n",
    "]\n",
    "\n",
    "for endpoint in model_endpoints_to_test:\n",
    "    # Identify the evaluation inside MLflow using a Run name.  run_name is a user-defined string.\n",
    "    with mlflow.start_run(run_name=endpoint):\n",
    "        # Change config to use the LLM\n",
    "        agent_config.llm_config.llm_endpoint_name = endpoint\n",
    "        # rag_agent = FunctionCallingAgent(agent_config=agent_config.dict())\n",
    "\n",
    "        # Log agent's code & config to MLflow\n",
    "        logged_agent_info = log_agent_to_mlflow(\n",
    "            agent_config=agent_config,\n",
    "            agent_code_file=os.path.join(\n",
    "                os.getcwd(), \"function_calling_agent_openai_sdk\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # def wrapper_fn(model_input: Dict[str, Any]):\n",
    "        #     return rag_agent.predict(model_input=model_input)\n",
    "\n",
    "        # Call Agent Evaluation.\n",
    "        result = mlflow.evaluate(\n",
    "            data=synthetic_data_df,  # Your evaluation set\n",
    "            model=logged_agent_info.model_uri,  # MLflow logged agent\n",
    "            model_type=\"databricks-agent\",  # Enable Mosaic AI Agent Evaluation\n",
    "\n",
    "            ## optional parameters below\n",
    "            extra_metrics=[no_pii],\n",
    "            # Optional, configure which LLM judges run.  By default, we run the relevant judges.\n",
    "            evaluator_config={\n",
    "                \"databricks-agent\": {\n",
    "                    \"metrics\": [\n",
    "                        \"chunk_relevance\",\n",
    "                        \"context_sufficiency\",\n",
    "                        \"correctness\",\n",
    "                        \"groundedness\",\n",
    "                        \"relevance_to_query\",\n",
    "                        \"safety\",\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "206f2c44-98e6-4e49-8c38-b9254253054b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "DEMO SCRIPT: You can see how you'd iterate to reach your production quality target.  For the purposes of the demo, let's assume one of the models above met your targets for either production or sharing it with internal stakeholders to test.  \n",
    "\n",
    "The process for deployment is the same - you'll register the Agent to Unity Catalog, and then call `agents.deploy(...)`.  From this command, you'll get production-ready REST API and a hosted web app - the Review App - that your stakeholders can use to test the model.  All logs and feedback are stored in an Inference Table along with the MLflow Trace, so you can debug any quality issues without needing to resort to spreadsheets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e78a8c3-b7d0-4f24-a2ac-629861b83068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import mlflow\n",
    "\n",
    "# You can log a new version or deploy an already logged/evaluated model from above.  Here, we use the last model logged for simplicity.\n",
    "\n",
    "# Use Unity Catalog as the model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Register the Agent's model to the Unity Catalog\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, \n",
    "    name=uc_model_name # Unity Catalog model is configured in settings cell\n",
    ")\n",
    "\n",
    "# Deploy to enable the Review App and create an API endpoint\n",
    "deployment_info = agents.deploy(uc_model_name, uc_registered_model_info.version)\n",
    "\n",
    "displayHTML(\n",
    "    f'<a href=\"{deployment_info.review_app_url}\" target=\"_blank\"><button style=\"color: white; background-color: #0073e6; padding: 10px 24px; cursor: pointer; border: none; border-radius: 4px;\">SME Chat UI (review app)</button></a>'\n",
    ")\n",
    "\n",
    "displayHTML(\n",
    "    f'<a href=\"{deployment_info.endpoint_url}\" target=\"_blank\"><button style=\"color: white; background-color: #0073e6; padding: 10px 24px; cursor: pointer; border: none; border-radius: 4px;\">Model Serving REST API</button></a>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83cd31e0-e63c-4295-8eec-91072b39cf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WARNING: Below steps are an early private preview.  Functionality may break, make sure you test BEFORE the customer demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e01da0e-8bfa-4086-8427-fcf3f2de67b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "DEMO SCRIPT:  While Synthetic data is great for unblocking your quality iteration, it is not perfect, and our best practice is to have SMEs review the generated data for accuracy.\n",
    "\n",
    "Here, I'm going to show you use our SME Evaluation Set Review UI - this is a gamified experience that is designed to allow your SMEs to efficiently review the evaluation set.  Since we give the SMEs a starting point - they can focus on \"reviewing\" vs. \"generating\" - as I'm sure you've experienced, its much easier to critique someone else's document than write one from scratch!\n",
    "\n",
    "Let's walk through this UI.  As the SMEs review each question, Agent Evaluation's backend automatically tracks the history and lineage and updates the Delta Table, so you can start using the reviewed data in parallel with the SME finishing their review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ed1763-98e6-4301-93d7-1d93121fca10",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optionally, show create managed evaluation setSDK for"
    }
   },
   "outputs": [],
   "source": [
    "import managed_evals as agent_evaluation_preview\n",
    "import copy\n",
    "\n",
    "# Delete to reset state\n",
    "# agent_evaluation_preview.delete_evals_table(evals_table_name=managed_eval_delta_table)\n",
    "\n",
    "# Create the managed evaluation set backend\n",
    "agent_evaluation_preview.create_evals_table(\n",
    "    # Delta Table where the managed evaluation set is stored\n",
    "    evals_table_name=managed_eval_delta_table,\n",
    "    # Generations from the deployed agent is used to improve the SME-facing UX for review the evaluation set\n",
    "    model_serving_endpoint_name=deployment_info.endpoint_name,\n",
    "    # Note: The mode parameter will be removed in future versions and replaced with a single mode\n",
    "    eval_mode=\"grading_notes\",\n",
    ")\n",
    "\n",
    "\n",
    "# Below is temporary code required to translate the synthetic evaluation set into the managed evaluation backend.  This is a temporary state - managed eval sets will soon support directly loading the synthetic data.\n",
    "\n",
    "\n",
    "# Load synthetic data\n",
    "df_dict = spark.table(synthetic_evaluation_set_delta_table).toPandas().to_dict(orient='records')\n",
    "\n",
    "# Translate to format accepted \n",
    "new_evals = []\n",
    "for row in df_dict:\n",
    "    new_row = copy.deepcopy(row)\n",
    "    new_row[\"expected_facts\"][0] = f\"- {new_row['expected_facts'][0]}\"\n",
    "    fact_list = \"\\n- \".join(new_row[\"expected_facts\"])\n",
    "\n",
    "    new_eval = {\n",
    "        \"request_id\": new_row[\"request_id\"],\n",
    "        \"request\": new_row[\"request\"],\n",
    "        \"grading_notes\": f\"The answer should mention the following facts either explicitly or implicitly (NOTE: It is sufficient for the answer to mention these facts **implicitly** because explicit agreement is not required!):\\n{fact_list}\",\n",
    "    }\n",
    "    new_evals.append(new_eval)\n",
    "\n",
    "\n",
    "\n",
    "agent_evaluation_preview.add_evals(evals=new_evals, evals_table_name=managed_eval_delta_table)\n",
    "\n",
    "\n",
    "## Get the link\n",
    "\n",
    "sme_ui_link = agent_evaluation_preview.get_evals_link(\n",
    "    evals_table_name=managed_eval_delta_table\n",
    ")\n",
    "displayHTML(\n",
    "    f'<a href=\"{sme_ui_link}/review\" target=\"_blank\"><button style=\"color: white; background-color: #0073e6; padding: 10px 24px; cursor: pointer; border: none; border-radius: 4px;\">SME Evaluation Set Review UI</button></a>'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "4. Pre-production (old)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
