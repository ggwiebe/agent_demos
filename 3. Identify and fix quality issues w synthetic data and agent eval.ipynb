{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5366137-d5c9-460c-9dcc-1e485f16a4ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq -r requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9091236a-5ee7-4b44-8567-fd6d3d32709e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cb93ee-ec98-4287-93e1-706b3391c701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs_df = (\n",
    "    spark.table(\"agents_demo.data.product_docs\")\n",
    "    .withColumnRenamed(\"indexed_doc\", \"content\")\n",
    "    .withColumnRenamed(\"product_id\", \"doc_uri\")\n",
    ")\n",
    "display(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc66dc18-1a9f-4548-8f70-7d725753df5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_json, struct, expr, lit\n",
    "from databricks.agents.eval import generate_evals_df\n",
    "\n",
    "# Optional guideline\n",
    "guidelines = \"\"\"\n",
    "# Task Description\n",
    "You are generating an evaluation dataset which will be used to test a customer analytics chatbot on its ability to answer business user's questions about our product catalog.\n",
    "\n",
    "# Content Guidelines\n",
    "- Address scenarios that customer support agents may face while helping customers understand our products.\n",
    "- Address scenarios that business analysts, who aren't familar with all of our products, may have\n",
    "\n",
    "# Example questions\n",
    "- how to troubleshoot <some issue>?\n",
    "- how many colors of <product>f are there?\n",
    "- what's the best product for <use case>?\n",
    "- did we change <feature> recently?\n",
    "\n",
    "# Style Guidelines\n",
    "- Questions should be succinct, and human-like.\n",
    "\n",
    "# Personas\n",
    "- A business analyst\n",
    "- A customer support agent\n",
    "\"\"\"\n",
    "\n",
    "# Generate 1 question for each document\n",
    "synthetic_eval_data = generate_evals_df(\n",
    "    docs=docs_df.head(20),\n",
    "    guidelines=guidelines, \n",
    "    num_questions_per_doc=1\n",
    ")\n",
    "\n",
    "display(synthetic_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741bd650-e8c6-4d4c-91e2-a956e2ea7601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Vibe check agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afa3a27-1dd4-4dab-9b50-64e9b8ae14b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent's code"
    }
   },
   "outputs": [],
   "source": [
    "%run ./function_calling_agent_openai_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0d62b2-84fa-4343-8c2b-9996b8c50c06",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent's config"
    }
   },
   "outputs": [],
   "source": [
    "# Pydantic class to make configuration easiser to use.  Developers can use this, Python dictionaries or YAML files for their configuration.\n",
    "from configs import (\n",
    "    AgentConfig,\n",
    "    FunctionCallingLLMConfig,\n",
    "    LLMParametersConfig,\n",
    "    RetrieverToolConfig,\n",
    "    RetrieverParametersConfig,\n",
    "    RetrieverSchemaConfig,\n",
    "    UCToolConfig,\n",
    ")\n",
    "import yaml\n",
    "\n",
    "\n",
    "docs_retriever = RetrieverToolConfig(\n",
    "    vector_search_index=\"agents_demo.data.product_docs_index\",  # UC Vector Search index\n",
    "    vector_search_schema=RetrieverSchemaConfig(\n",
    "        primary_key=\"product_id\",\n",
    "        chunk_text=\"indexed_doc\",\n",
    "        document_uri=\"product_id\",\n",
    "        additional_metadata_columns=[\n",
    "            # \"issue_area\",\n",
    "            # \"issue_category\",\n",
    "            # \"issue_sub_category\",\n",
    "            # \"product_category\",\n",
    "            # \"product_sub_category\",\n",
    "            # \"conversation\",\n",
    "            # \"timestamp\",\n",
    "            # \"user_id\",\n",
    "        ],\n",
    "    ),\n",
    "    vector_search_parameters=RetrieverParametersConfig(\n",
    "        num_results=1,\n",
    "        query_type=\"ann\",  # Type of search: ann or hybrid\n",
    "    ),\n",
    "    vector_search_threshold=0.0,\n",
    "    # Tool prompt templates\n",
    "    # chunk_template=\"Passage text: {chunk_text}\\nPassage metadata: {metadata}\\n\\n\",\n",
    "    # prompt_template=\"\"\"Use the following pieces of retrieved context to answer the question.\\nOnly use the passages from context that are relevant to the query to answer the question, ignore the irrelevant passages.  When responding, cite your source, referring to the passage by the columns in the passage's metadata.\\n\\nContext: {context}\"\"\",\n",
    "    retriever_query_parameter_prompt=\"query to look up in the product documentation\",\n",
    "    retriever_filter_parameter_prompt=\"Optional filters to apply to the search. An array of objects, each specifying a field name and the filters to apply to that field.  ONLY use the LIKE type of filter if you have a string to query in product_category, etc.  Prefer to query WITHOUT filters.\",\n",
    "    tool_description_prompt=\"Search the production documentation for product information.  If you need to know how to troubleshoot, what a product does, common issues, etc, use this tool.\",\n",
    "    tool_name=\"retrieve_product_docs\",\n",
    "    # Retriever internals\n",
    "    tool_class_name=\"VectorSearchRetriever\",\n",
    ")\n",
    "\n",
    "# python_exec_config = UCToolConfig(\n",
    "#     uc_catalog_name=\"ep\",\n",
    "#     uc_schema_name=\"agent_demo\",\n",
    "#     uc_function_name=\"python_exec\",\n",
    "# )\n",
    "\n",
    "# recent_orders = UCToolConfig(\n",
    "#     uc_catalog_name=\"ep\",\n",
    "#     uc_schema_name=\"agent_demo\",\n",
    "#     uc_function_name=\"user_orders\",\n",
    "# )\n",
    "\n",
    "########################\n",
    "#### ✅✏️ LLM configuration\n",
    "########################\n",
    "\n",
    "llm_config = FunctionCallingLLMConfig(\n",
    "    llm_endpoint_name=\"agents-demo-gpt4o\",  # Model serving endpoint\n",
    "    llm_system_prompt_template=(\n",
    "        \"\"\"You are a helpful assistant that answers questions by calling tools.  Provide responses ONLY based on the outputs from tools.  Ask follow up questions if needed.  If don't get relevant results from the retriever tool while using filters, try to call the retriever tool again with JUST a query and no filters!\"\"\"\n",
    "    ),  # System prompt template\n",
    "    llm_parameters=LLMParametersConfig(\n",
    "        temperature=0.01, max_tokens=1500\n",
    "    ),  # LLM parameters\n",
    "    tools=[docs_retriever],\n",
    ")\n",
    "\n",
    "function_calling_agent_config = AgentConfig(\n",
    "    llm_config=llm_config,\n",
    "    input_example={\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the top customer issue?\",\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    agent_description=\"Has access to the product documentation, transcripts from our customer service call center and information about customer's recent orders.\",\n",
    "    agent_name=\"CustomerServiceTranscripts\",\n",
    "    endpoint_name=\"agents_ep-agent_demo-customer_bot_function_calling_agent\",\n",
    ")\n",
    "\n",
    "with open(\"config.yml\", \"w\") as file:\n",
    "    yaml.dump(function_calling_agent_config.dict(), file, default_flow_style=False)\n",
    "\n",
    "import json\n",
    "print(json.dumps(function_calling_agent_config.dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bf7d98e-4c22-4c62-b8f7-47acb46e9ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agent = FunctionCallingAgent(agent_config=function_calling_agent_config.dict())\n",
    "\n",
    "response = agent.predict(\n",
    "    model_input={\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"How does our blender work?\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39bfa5f8-ebef-482a-ab7d-74ce8bab34d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial quality evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1862bea0-fc61-4fc3-adc0-a0ca2f599600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.types.llm import CHAT_MODEL_INPUT_SCHEMA\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.models.rag_signatures import StringResponse\n",
    "\n",
    "with mlflow.start_run(run_name=\"initial_agent\"):\n",
    "    # Log to MLflow\n",
    "    agent_model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"function_calling_agent_openai_sdk\",  # Agent's code\n",
    "        model_config=function_calling_agent_config.dict(),  # Agent's config\n",
    "        artifact_path=\"agent\",\n",
    "        input_example=function_calling_agent_config.input_example,\n",
    "        signature=ModelSignature(  # Agent's schema\n",
    "            inputs=CHAT_MODEL_INPUT_SCHEMA,\n",
    "            outputs=StringResponse(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Run the agent for these queries, using Agent evaluation to parallelize the calls\n",
    "    eval_results = mlflow.evaluate(\n",
    "        model=agent_model_info.model_uri,  # run the logged Agent for evaluation\n",
    "        data=synthetic_eval_data,  # Eval set\n",
    "        model_type=\"databricks-agent\",  # use Agent Evaluation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72cb7ba-6c25-4c26-b8fd-09625d9ac6c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Make a change to fix retrieval quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817031ac-2640-468a-8e2e-6f725dcfa9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make some prompt changes to encourage the LLM to be less restrictive with filtering\n",
    "\n",
    "# Original\n",
    "function_calling_agent_config.llm_config.llm_system_prompt_template = \"\"\"\n",
    "You are a helpful assistant that answers questions by calling tools.  Provide responses ONLY based on the outputs from tools.  Ask follow up questions if needed.  If you try the retriever tool WITH filters, and don't get any results, try again without filters for the product categories, only with the user_id.\"\"\"\n",
    "# New\n",
    "function_calling_agent_config.llm_config.llm_system_prompt_template = \"You are a helpful assistant that answers questions by calling tools.  Provide responses ONLY based on the outputs from tools.  Ask follow up questions if needed.  If don't get relevant results from the retriever tool while using filters, try to call the retriever tool again with JUST a query and no filters!\"\n",
    "\n",
    "# Get the retriever tool\n",
    "product_doc_retriever = next(\n",
    "    (tool for tool in function_calling_agent_config.llm_config.tools \n",
    "     if isinstance(tool, RetrieverToolConfig) and tool.vector_search_index == \"agents_demo.data.product_docs_index\"), \n",
    "    None\n",
    ")\n",
    "# Original\n",
    "product_doc_retriever.retriever_filter_parameter_prompt = \"Optional filters to apply to the search. An array of objects, each specifying a field name and the filters to apply to that field.  ONLY use the LIKE type of filter if you have a string to query in product_category, etc.\"\n",
    "\n",
    "# New\n",
    "product_doc_retriever.retriever_filter_parameter_prompt = \"Optional filters to apply to the search. An array of objects, each specifying a field name and the filters to apply to that field.  ONLY use the LIKE type of filter if you have a string to query in product_category, etc.  Prefer to query WITHOUT filters.\"\n",
    "\n",
    "product_doc_retriever.vector_search_parameters.num_results = 5\n",
    "\n",
    "# Re-run the logging code from the cell above\n",
    "# agent_model_info = log_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1547caf-8b9a-465b-bd68-d6c4e80c5cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46a4f33-df1b-4f49-b752-e811fb5d7d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"changed_prompt\"):\n",
    "    # Log to MLflow\n",
    "    agent_model_info = mlflow.pyfunc.log_model(\n",
    "        python_model=\"function_calling_agent_openai_sdk\",  # Agent's code\n",
    "        model_config=function_calling_agent_config.dict(),  # Agent's config\n",
    "        artifact_path=\"agent\",\n",
    "        input_example=function_calling_agent_config.input_example,\n",
    "        signature=ModelSignature(  # Agent's schema\n",
    "            inputs=CHAT_MODEL_INPUT_SCHEMA,\n",
    "            outputs=StringResponse(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Run the agent for these queries, using Agent evaluation to parallelize the calls\n",
    "    eval_results = mlflow.evaluate(\n",
    "        model=agent_model_info.model_uri,  # run the logged Agent for evaluation\n",
    "        data=synthetic_eval_data,  # Eval set\n",
    "        model_type=\"databricks-agent\",  # use Agent Evaluation\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2716391597725216,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "3. Identify and fix quality issues w synthetic data and agent eval",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
