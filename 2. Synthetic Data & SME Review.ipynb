{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5366137-d5c9-460c-9dcc-1e485f16a4ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq \"git+https://github.com/mlflow/mlflow.git@master\" \"https://ml-team-public-read.s3.us-west-2.amazonaws.com/wheels/rag-studio/staging/databricks_agents-0.8.1.dev0-py3-none-any.whl\" \"https://ml-team-public-read.s3.us-west-2.amazonaws.com/wheels/managed-evals/staging/databricks_managed_evals-latest-py3-none-any.whl\" databricks-vectorsearch databricks-sdk[openai]  -r requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdf666a-9b16-44f2-852f-bb741ede82cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['RAG_EVAL_MAX_INPUT_ROWS'] = '2000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a3c914-2adb-447a-ac91-34c27b8e526d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "from mlflow.metrics.genai import make_genai_metric_from_prompt\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksVectorSearchIndex,\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksGenieSpace,\n",
    "    DatabricksFunction\n",
    ")\n",
    "from mlflow.types.llm import CHAT_MODEL_INPUT_SCHEMA\n",
    "from mlflow.models.signature import ModelSignature, Schema\n",
    "from mlflow.models.rag_signatures import StringResponse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9091236a-5ee7-4b44-8567-fd6d3d32709e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cb93ee-ec98-4287-93e1-706b3391c701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs_df = (\n",
    "    spark.table(\"agents_demo.default.product_docs\")\n",
    "    .withColumnRenamed(\"indexed_doc\", \"content\")\n",
    "    .withColumnRenamed(\"product_id\", \"doc_uri\")\n",
    ")\n",
    "display(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc66dc18-1a9f-4548-8f70-7d725753df5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_json, struct, expr, lit\n",
    "from databricks.agents.eval import generate_evals_df\n",
    "\n",
    "# Optional guideline\n",
    "guidelines = \"\"\"\n",
    "# Task Description\n",
    "You are generating an evaluation dataset which will be used to test a customer analytics chatbot on its ability to answer business user's questions about our product catalog.\n",
    "\n",
    "# Content Guidelines\n",
    "- Address scenarios that customer support agents may face while helping customers understand our products.\n",
    "- Address scenarios that business analysts, who aren't familar with all of our products, may have\n",
    "\n",
    "# Example questions\n",
    "- how to troubleshoot <some issue>?\n",
    "- how many colors of <product> are there?\n",
    "- what's the best product for <use case>?\n",
    "- did we change <feature> recently?\n",
    "\n",
    "# Style Guidelines\n",
    "- Questions should be succinct, and human-like.\n",
    "\n",
    "# Personas\n",
    "- A business analyst\n",
    "- A customer support agent\n",
    "\"\"\"\n",
    "\n",
    "# Generate 1 question for each document\n",
    "synthetic_eval_data = generate_evals_df(\n",
    "    docs=docs_df.limit(10),\n",
    "    guidelines=guidelines, \n",
    "    num_questions_per_doc=1\n",
    ")\n",
    "\n",
    "display(synthetic_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f3f70e-a716-4ed5-a81a-dd5947c02f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import managed_evals\n",
    "\n",
    "managed_eval_delta_table = \"agents_demo.default.managed_evaluation_set\"\n",
    "\n",
    "# Add synthetic evals\n",
    "managed_evals.add_evals(\n",
    "    evals=synthetic_eval_data, evals_table_name=managed_eval_delta_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c5444b-90c0-486e-b350-5c902924b32c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sme_ui_link = managed_evals.get_evals_link(\n",
    "    evals_table_name=managed_eval_delta_table\n",
    ")\n",
    "displayHTML(\n",
    "    f'<a href=\"{sme_ui_link}/review\" target=\"_blank\"><button style=\"color: white; background-color: #0073e6; padding: 10px 24px; cursor: pointer; border: none; border-radius: 4px;\">SME Evaluation Set Review UI</button></a>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9818c4fb-07a6-46a2-88c7-0bcdf1b3cff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c022cd7d-3d96-480e-93f4-ff2ced34e144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import managed_evals\n",
    "\n",
    "managed_eval_delta_table = \"agents_demo.default.managed_evaluation_set\"\n",
    "\n",
    "# Create the managed evaluation set backend\n",
    "managed_evals.create_evals_table(\n",
    "    # Delta Table where the managed evaluation set is stored\n",
    "    evals_table_name=managed_eval_delta_table,\n",
    "    # Generations from the deployed agent is used to improve the SME-facing UX for review the evaluation set\n",
    "    model_serving_endpoint_name=\"agents_agents_demo-default-product_docs_agent\",\n",
    "    # Note: The mode parameter will be removed in future versions and replaced with a single mode\n",
    "    eval_mode=\"grading_notes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362f0194-dd90-483e-ae62-fccff735a1a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#managed_evals.delete_evals_table(evals_table_name=managed_eval_delta_table)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2716391597726518,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Synthetic Data & SME Review",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
